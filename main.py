import os, time, requests, threading
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# === æª”æ¡ˆæ—¥èªŒï¼š/tmp/worker.log ===
import logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s %(levelname)s: %(message)s',
    handlers=[
        logging.FileHandler('/tmp/worker.log'),
        logging.StreamHandler()
    ]
)
log = logging.getLogger(__name__)

DB_URL  = os.getenv('DB_URL')  # https://shopee-checker-i3ip.onrender.com/api/products
BATCH   = int(os.getenv('BATCH', 20))
API_KEY = os.getenv('API_KEY')

sess = requests.Session()
retries = Retry(total=3, backoff_factor=2, status_forcelist=[502, 503, 504])
sess.mount('https://', HTTPAdapter(max_retries=retries))

# ç°¡å–®çš„ API è«‹æ±‚é ­éƒ¨
headers = {
    'User-Agent': 'ShopeeChecker/1.0',
    'Accept': 'application/json',
    'Accept-Encoding': 'identity'
}
sess.headers.update(headers)

def update_status(row_id, status):
    """åªæ›´æ–°å¤±æ•ˆçš„å•†å“ç‹€æ…‹"""
    try:
        if status == 'å¤±æ•ˆ':
            data = {'id': row_id, 'status': status}
            sess.post(DB_URL, data=data, timeout=30)
            log.info("âœ… Recorded removed product: id=%s", row_id)
        # æœ‰æ•ˆçš„å•†å“ä¸æ›´æ–°ï¼Œä¿æŒé»˜èªç‹€æ…‹
    except Exception as e:
        log.error("update_status failed: %s", e)

def job():
    log.info("worker started - DB_URL: %s", DB_URL)
    while True:
        for attempt in range(3):
            try:
                params = {'limit': BATCH}
                res = sess.get(DB_URL, params=params, timeout=30)
                
                if res.status_code != 200:
                    log.warning("HTTP %s from API (attempt %s)", res.status_code, attempt+1)
                    time.sleep(5)
                    continue
                    
                if not res.text.strip():
                    log.warning("API returned empty body (attempt %s)", attempt+1)
                    time.sleep(5)
                    continue
                    
                try:
                    rows = res.json()
                    log.info("ğŸ” Checking %s products", len(rows))
                    break
                except Exception as e:
                    log.error("Invalid JSON from API (attempt %s): %s", attempt+1, e)
                    time.sleep(5)
                    continue
                    
            except Exception as e:
                log.warning("fetch attempt %s failed: %s", attempt+1, e)
                time.sleep(5)
        else:
            log.error("fetch failed 3 times, skip cycle")
            time.sleep(30)
            continue

        if not rows:
            log.info("ğŸ“­ No products to check, sleep 5min")
            time.sleep(300)
            continue

        for r in rows:
            url = r['real_url']
            log.info("ğŸ” Checking product: %s", url)
            
            api = f'https://api.scrapingant.com/v2/general?url={url}&x-api-key={API_KEY}&wait_for_selector=.product-not-exist__text'
            try:
                html = sess.get(api, timeout=30).text
                
                # æ”¹é€²çš„ä¸‹æ¶æª¢æ¸¬é‚è¼¯
                removed_indicators = [
                    'product-not-exist__text',
                    'å•†å“å·²ä¸‹æ¶',
                    'å·²çµæŸè²©å”®',
                    'å·²ä¸‹æ¶',
                    'å•†å“ä¸å­˜åœ¨',
                    'This product is no available',
                    'product-not-available'
                ]
                
                is_removed = any(indicator in html for indicator in removed_indicators)
                
                if is_removed:
                    status = 'å¤±æ•ˆ'
                    log.warning("ğŸš« Product removed: %s", url)
                    # è¨˜éŒ„ä¸‹æ¶å•†å“çš„è©³ç´°ä¿¡æ¯ç”¨æ–¼èª¿è©¦
                    for indicator in removed_indicators:
                        if indicator in html:
                            log.info("ğŸ“ Found removal indicator: %s", indicator)
                            break
                else:
                    status = 'æœ‰æ•ˆ'
                    log.info("âœ… Product active: %s", url)
                
                update_status(r['id'], status)
                
            except Exception as e:
                log.error("scrapingant error for %s: %s", url, e)
                continue

        log.info("ğŸ”„ Batch completed, wait 30s")
        time.sleep(30)

if __name__ == '__main__':
    job()