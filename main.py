import os, time, requests, threading
import urllib.parse
import html as html_parser
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# === æª”æ¡ˆæ—¥èªŒï¼š/tmp/worker.log ===
import logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s %(levelname)s: %(message)s',
    handlers=[
        logging.FileHandler('/tmp/worker.log'),
        logging.StreamHandler()
    ]
)
log = logging.getLogger(__name__)

DB_URL  = os.getenv('DB_URL')
BATCH   = int(os.getenv('BATCH', 20))
API_KEY = os.getenv('API_KEY')

sess = requests.Session()
retries = Retry(total=3, backoff_factor=2, status_forcelist=[502, 503, 504])
sess.mount('https://', HTTPAdapter(max_retries=retries))

headers = {
    'User-Agent': 'ShopeeChecker/1.0',
    'Accept': 'application/json',
    'Accept-Encoding': 'identity'
}
sess.headers.update(headers)

def update_status(row_id, status):
    """åªæ›´æ–°å¤±æ•ˆçš„å•†å“ç‹€æ…‹"""
    try:
        if status == 'å¤±æ•ˆ':
            data = {'id': row_id, 'status': status}
            sess.post(DB_URL, data=data, timeout=30)
            log.info("âœ… Recorded removed product: id=%s", row_id)
    except Exception as e:
        log.error("update_status failed: %s", e)

def check_removed(html):
    """æª¢æ¸¬å•†å“æ˜¯å¦ä¸‹æ¶ - åªæª¢æ¸¬ç¢ºåˆ‡æ¨™èªŒ"""
    # è¨˜éŒ„éƒ¨åˆ†HTMLç”¨æ–¼é™¤éŒ¯
    html_sample = html[:200] if len(html) > 200 else html
    log.debug("HTML sample: %s", html_sample)
    
    # æ–¹æ³•1: ç›´æ¥æª¢æ¸¬ç¢ºåˆ‡æ¨™èªŒ
    if 'æ­¤å•†å“ä¸å­˜åœ¨' in html:
        log.info("ğŸ¯ Detected 'æ­¤å•†å“ä¸å­˜åœ¨' in raw HTML")
        return True
        
    # æ–¹æ³•2: URLè§£ç¢¼å¾Œæª¢æ¸¬
    try:
        decoded_html = urllib.parse.unquote(html)
        if 'æ­¤å•†å“ä¸å­˜åœ¨' in decoded_html:
            log.info("ğŸ¯ Detected 'æ­¤å•†å“ä¸å­˜åœ¨' in URL decoded HTML")
            return True
    except Exception as e:
        log.debug("URL decode failed: %s", e)
        
    # æ–¹æ³•3: HTMLå¯¦é«”è§£ç¢¼å¾Œæª¢æ¸¬
    try:
        decoded_html = html_parser.unescape(html)
        if 'æ­¤å•†å“ä¸å­˜åœ¨' in decoded_html:
            log.info("ğŸ¯ Detected 'æ­¤å•†å“ä¸å­˜åœ¨' in HTML entity decoded HTML")
            return True
    except Exception as e:
        log.debug("HTML entity decode failed: %s", e)
    
    # åªæª¢æ¸¬ç¢ºåˆ‡çš„è¦çš®ä¸‹æ¶æ¨™èªŒï¼Œç§»é™¤æ¨¡ç³Šçš„éŒ¯èª¤æ¨™èªŒ
    # é€™æ¨£å¯ä»¥é¿å…èª¤åˆ¤æ­£å¸¸å•†å“
    
    return False

def job():
    log.info("worker started - DB_URL: %s", DB_URL)
    
    # å–®æ¬¡åŸ·è¡Œï¼Œç§»é™¤ while True å¾ªç’°
    for attempt in range(3):
        try:
            params = {'limit': BATCH}
            res = sess.get(DB_URL, params=params, timeout=30)
            
            if res.status_code != 200:
                log.warning("HTTP %s from API (attempt %s)", res.status_code, attempt+1)
                time.sleep(5)
                continue
                
            try:
                rows = res.json()
                log.info("ğŸ” Checking %s products", len(rows))
                break
            except Exception as e:
                log.error("Invalid JSON from API (attempt %s): %s", attempt+1, e)
                time.sleep(5)
                continue
                
        except Exception as e:
            log.warning("fetch attempt %s failed: %s", attempt+1, e)
            time.sleep(5)
    else:
        log.error("fetch failed 3 times, skip cycle")
        return

    if not rows:
        log.info("ğŸ“­ No products to check")
        return

    removed_count = 0
    active_count = 0
    
    for r in rows:
        url = r['real_url']
        log.info("ğŸ” Checking product: %s", url)
        
        # å¢åŠ  browser_wait=8000 è®“è¦çš®JSæœ‰è¶³å¤ æ™‚é–“åŸ·è¡Œ
        api = f'https://api.scrapingant.com/v2/general?url={url}&x-api-key={API_KEY}&browser=true&browser_wait=8000'
        
        try:
            response = sess.get(api, timeout=90)  # å¢åŠ è¶…æ™‚æ™‚é–“
            html = response.text
            
            # ä½¿ç”¨ç²¾ç¢ºçš„ä¸‹æ¶æª¢æ¸¬
            if check_removed(html):
                status = 'å¤±æ•ˆ'
                removed_count += 1
                log.warning("ğŸš« Product REMOVED: %s", url)
            else:
                status = 'æœ‰æ•ˆ'
                active_count += 1
                log.info("âœ… Product ACTIVE: %s", url)
            
            update_status(r['id'], status)
            
        except Exception as e:
            log.error("scrapingant error for %s: %s", url, e)
            continue

    log.info("ğŸ“Š Check completed: %s active, %s removed", active_count, removed_count)
    log.info("âœ… Single batch completed - exiting")

if __name__ == '__main__':
    job()